{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a270a839",
   "metadata": {},
   "source": [
    "## Load Synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3287e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from interfacegan.models.stylegan_generator import StyleGANGenerator\n",
    "from models.image_to_latent import ImageToLatent\n",
    "from models.latent_optimizer import LatentOptimizer, VGGProcessing, PostSynthesisProcessing\n",
    "from models.losses import LatentLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad079cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "image_shape = (image_size, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dd6804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthesizer generates images from latents\n",
    "synthesizer_name = 'stylegan_ffhq'\n",
    "synthesizer = StyleGANGenerator(synthesizer_name).model.synthesis\n",
    "\n",
    "# to clip the values of the generated image\n",
    "post_processor = PostSynthesisProcessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf82625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize(latent):\n",
    "    \"\"\" Synthesize an image from the WP latents.\n",
    "    \n",
    "    Args:\n",
    "        latent: WP latent as numpy array of shape (batch_size, 18, 512)\n",
    "        \n",
    "    Returns:\n",
    "        synthesized image as PIL.Image of shape (256, 256)\n",
    "    \"\"\"\n",
    "    latent = torch.from_numpy(latent.astype(np.float32))\n",
    "    synth = synthesizer(latent)\n",
    "    postproc = post_processor(synth).detach().numpy().astype(np.uint8)[0].transpose((1, 2, 0))\n",
    "    from_tensor = transforms.ToPILImage()\n",
    "    output_image = from_tensor(postproc)\n",
    "    output_image.thumbnail(image_shape)\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb90061",
   "metadata": {},
   "source": [
    "## Validate Data\n",
    "\n",
    "Make sure that image generated from a given dlatent looks the same as the corresponding image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('data/stylegan_ffhq')\n",
    "\n",
    "dlatents = np.load(data_dir/'dlatents/wp.npy')\n",
    "dlatents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62c3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "img = Image.open(f'data/stylegan_ffhq/images/{idx + 10000:06d}.jpg')\n",
    "img.thumbnail(image_shape)\n",
    "\n",
    "# Image.fromarray(np.interp(dlatent, (dlatent.min(), dlatent.max()), (0, 255)).astype(np.uint8))\n",
    "\n",
    "display(img, synthesize(dlatents[np.newaxis, idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cea0d7",
   "metadata": {},
   "source": [
    "## Load Latent Optimizer\n",
    "\n",
    "Optionally load the `ImageToLatent` network that generates a latent representation of the image to initialize the optimization with instead of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_layer = 12\n",
    "learning_rate = 1\n",
    "iterations = 100\n",
    "# model_path = '2022-11-17_13:17_1000.pt'\n",
    "model_path = '2022-11-20_11 06_50000_19.pt'\n",
    "\n",
    "# preprocessing for VGG and ImageToLatent nets\n",
    "vgg_processing = VGGProcessing()\n",
    "\n",
    "# image to latent creates an initial latent to start the optimization from \n",
    "image_to_latent = ImageToLatent(image_size)\n",
    "checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "image_to_latent.load_state_dict(checkpoint['model_state_dict'])\n",
    "# image_to_latent.load_state_dict(checkpoint)\n",
    "image_to_latent.eval()\n",
    "\n",
    "# latent optimizer iteratively improves the latent of the image using embeddings from vgg  \n",
    "latent_optimizer = LatentOptimizer(synthesizer, vgg_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887eb3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_latents(\n",
    "    image,\n",
    "    use_image_to_latent=False,\n",
    "    iterations=iterations,\n",
    "):\n",
    "    \"\"\" Optimize/train the latents of the image.\n",
    "    \n",
    "    Args:\n",
    "        image: input image as numpy array of shape (3, 256, 256)\n",
    "        use_image_to_latent: whether to generate the initial latent using ImageToLatent net\n",
    "        iterations: number of optimization iterations\n",
    "        \n",
    "    Returns:\n",
    "        optimized WP latents as numpy array of shape (1, 18, 512)\n",
    "    \"\"\"\n",
    "    # Optimize only the dlatents.\n",
    "    for param in latent_optimizer.parameters():\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "    image = torch.from_numpy(image)\n",
    "    image = latent_optimizer.vgg_processing(image)\n",
    "    reference_features = latent_optimizer.vgg16(image).detach()\n",
    "    image = image.detach()\n",
    "    \n",
    "    latents_to_be_optimized = (\n",
    "        image_to_latent(image.unsqueeze(0)).detach()\n",
    "        if use_image_to_latent\n",
    "        else torch.zeros((1, 18, 512))\n",
    "    )\n",
    "    latents_to_be_optimized = latents_to_be_optimized.requires_grad_(True)\n",
    "\n",
    "    criterion = LatentLoss()\n",
    "    optimizer = torch.optim.SGD([latents_to_be_optimized], lr=learning_rate)\n",
    "\n",
    "    progress_bar = tqdm(range(iterations))\n",
    "    for step in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        generated_image_features = latent_optimizer(latents_to_be_optimized).squeeze()\n",
    "        \n",
    "        loss = criterion(generated_image_features, reference_features)\n",
    "        loss.backward()\n",
    "        loss = loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        progress_bar.set_description(\"Step: {}, Loss: {}\".format(step, loss))\n",
    "    \n",
    "    optimized_dlatents = latents_to_be_optimized.detach().numpy()\n",
    "    \n",
    "    return optimized_dlatents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c4d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    image,\n",
    "    use_image_to_latent=False,\n",
    "    iterations=iterations,\n",
    "):\n",
    "    \"\"\" Optimize the latents and synthesize the image.\n",
    "    \n",
    "    Args:\n",
    "        image: input image as numpy array of shape (3, 256, 256)\n",
    "        use_image_to_latent: whether to generate the initial latent using ImageToLatent net\n",
    "        iterations: number of optimization iterations\n",
    "        \n",
    "    Returns:\n",
    "        image synthesized from the optimized latents\n",
    "    \"\"\"\n",
    "    image = image.transpose((2, 0, 1))\n",
    "    dlatents = optimize_latents(\n",
    "        image,\n",
    "        use_image_to_latent=use_image_to_latent,\n",
    "        iterations=iterations,\n",
    "    )\n",
    "    output_image = synthesize(dlatents)\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a059e415",
   "metadata": {},
   "source": [
    "## Use Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4dbc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2113759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = Image.open('data/us/maoka.png')\n",
    "image1.thumbnail(image_shape)\n",
    "image2 = Image.open('data/us/mamala.png')\n",
    "image2.thumbnail(image_shape)\n",
    "\n",
    "# vector in the WP latent space to move along to change the age\n",
    "age_boundary = np.load('InterFaceGAN/boundaries/stylegan_ffhq_age_w_boundary.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94529cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_offspring(\n",
    "    image1,\n",
    "    image2,\n",
    "    weight=0.5,\n",
    "    child_scale=1,\n",
    "):\n",
    "    images = []\n",
    "    for image in (image1, image2):\n",
    "        image = torch.from_numpy(np.asarray(image).transpose((2, 0, 1)))\n",
    "        image = vgg_processing(image).detach()\n",
    "        images.append(image)\n",
    "    images = torch.stack(images)\n",
    "    latents = image_to_latent(images).detach().numpy()\n",
    "    latent = (weight * latents[0] + (1 - weight) * latents[1]) - child_scale * age_boundary\n",
    "    offspring = synthesize(latent[np.newaxis])\n",
    "    return offspring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7906e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "?gr.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce802f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.Interface(\n",
    "    fn=predict_offspring,\n",
    "    inputs=[\n",
    "        gr.Image(\n",
    "            label='XY',\n",
    "            shape=image_shape,\n",
    "        ),\n",
    "        gr.Image(\n",
    "            label='XX',\n",
    "            shape=image_shape,\n",
    "        ),\n",
    "#         gr.Checkbox(\n",
    "# #             value=True,\n",
    "#             label='Use ImageToLatent',\n",
    "#         ),\n",
    "#         gr.Slider(\n",
    "#             minimum=0,\n",
    "#             maximum=500,\n",
    "#             value=0,\n",
    "#             label='Iterations',\n",
    "#         ),\n",
    "        gr.Slider(\n",
    "            label='XY weight',\n",
    "            minimum=0,\n",
    "            maximum=1,\n",
    "            value=.5,\n",
    "            step=.1,\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            label='child scale',\n",
    "            minimum=0,\n",
    "            maximum=3,\n",
    "            value=1.5,\n",
    "            step=.5,\n",
    "        )\n",
    "    ],\n",
    "    examples=[\n",
    "        [\n",
    "            'data/us/maoka.png',\n",
    "            'data/us/mamala.png',\n",
    "#             .5,\n",
    "#             2,\n",
    "        ],\n",
    "    ],\n",
    "    outputs=gr.Image(type='pil'),\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910630c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
